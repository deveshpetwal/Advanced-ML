---
title: "Assignment 4"
output: html_notebook:
---

#Usage of Checkpoints
Allow us to use a pre-trained model for inference without having to retrain the model
Resume the training process from where we left off in case it was interrupted or for fine-tuning the model

#Steps for saving and loading model and weights using checkpoint
Create the model
Specify the path where we want to save the checkpoint files
Create the callback function to save the model
Apply the callback function during the training
Evaluate the model on test data
Load the pre-trained weights on a new model using load_weights() or restoring the weights from the latest checkpoint

#Libraries Used:

```{r}
library(tibble)
library(readr)
library(keras)
```

#Reading Data:

```{r}
data <- read.csv("jena_climate_2009_2016.csv")
head(data)
```


#Preparing the data:

```{r}
data <- data.matrix(data[,-1])
train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)
```

#Generation Function:

```{r}
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1, 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }            
    
    list(samples, targets)
  }
}
```

#Variables:

```{r}
lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128
train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)
val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)
test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- (300001 - 200001 - lookback) / batch_size
  # This is how many steps to draw from `test_gen`
# in order to see the whole test set:
test_steps <- (nrow(data) - 300001 - lookback) / batch_size
```

#Model 1:

```{r}
model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)
model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)
history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 30,
  validation_data = val_gen,
  validation_steps = val_steps
)

plot(history)
```


#Model 2 with LSTM layer

```{r echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_lstm(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)
model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)
history2 <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 30,
  validation_data = val_gen,
  validation_steps = val_steps
)

plot(history2)
```

#Checkpoints and tensor board commands will be updated once the program runs on cloud.